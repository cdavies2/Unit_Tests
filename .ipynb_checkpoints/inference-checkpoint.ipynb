{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706f2c9f-8072-4f04-ac3e-dd1dcb2cf9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac.cdavies/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce58aed-7cb1-467c-a796-1cea508aa28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Loading checkpoint shards: 100%|██████| 2/2 [00:00<00:00,  4.51it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "def get_pipeline(model_id, **kwargs):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=128,\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "        },\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "pipelines = {\n",
    "    \"llama3\": get_pipeline(\"meta-llama/Llama-3.2-1B\"),\n",
    "    \"mistral\": get_pipeline(\"mistralai/Mistral-7B-v0.1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12785086-8398-4cfb-9b2e-883c01c59490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|█████████████| 2/2 [01:20<00:00, 40.46s/it]\n",
      "Loading checkpoint shards: 100%|██████| 2/2 [00:21<00:00, 10.79s/it]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m pipe\u001b[38;5;241m=\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant teaching at a university level,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m                Use technical and complex terms in your explanation \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease explain what a prompt injection is\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe(messages)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1621\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1621\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1624\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1789\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe=pipeline(\"text-generation\", \"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"\"\"You are an assistant teaching at a university level,\n",
    "                Use technical and complex terms in your explanation \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": \"Please explain what a prompt injection is\"}\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbb0a14-0035-4bb0-bf14-beb2bbce33f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████| 2/2 [00:24<00:00, 12.33s/it]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m pipe\u001b[38;5;241m=\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m     {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant teaching at a university level,\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m                Use technical and complex terms in your explanation \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m},\n\u001b[1;32m     12\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease explain what a prompt injection is\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     13\u001b[0m ]\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe(messages)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1621\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1621\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1624\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1789\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "#this is the chat template for when we chat with the Mistalai model\n",
    "#llaToken=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "#this is the chat template for the Llama3 model\n",
    "from transformers import pipeline\n",
    "pipe=pipeline(\"text-generation\", \"mistralai/Mistral-7B-v0.1\")\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"\"\"You are an assistant teaching at a university level,\n",
    "                Use technical and complex terms in your explanation \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": \"Please explain what a prompt injection is\"}\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(pipe(messages)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28342549-401c-44fd-818e-ae5ba7e66190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama3': [{'generated_text': \"Hello World! I am a graduate student at the University of Maryland studying mathematics. I have a PhD in Mathematics and a BS in Mathematics from the University of Wisconsin-Madison. My research interests are in algebraic combinatorics, combinatorial number theory, and computational algebraic geometry. I am interested in learning about other people's research and trying to contribute to the community by helping to write papers or giving talks.\\nI am a graduate student at the University of Maryland studying mathematics. I have a PhD in Mathematics and a BS in Mathematics from the University of Wisconsin-Madison. My research interests are in algebraic combinatorics, combinator\"}],\n",
       " 'mistral': [{'generated_text': 'Hello World!\\n\\nI’m a 20-something year old girl who loves to travel, eat, and take pictures. I’m a full-time student at the University of Florida, and I’m currently studying abroad in London. I’m a huge fan of the outdoors, and I love to hike, bike, and run. I’m also a huge fan of the beach, and I love to swim and surf. I’m a huge fan of music, and I love to listen to all kinds of music. I’m a huge fan of food, and I love to eat all kinds of'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ name: pipeline(\"Hello World!\") for name, pipeline in pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ef47a3-1b4e-4148-bf86-8f05b95d2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama3': [{'generated_text': 'Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\\nvalues are 0 and 1 respectively:\\n{\"a\": 0, \"b\": 1}\\nHere is an example of valid JSON object whose only keys are months in the\\nGregorian calendar and values are the number of days in those months. The\\nkeys are not case sensitive:\\n{\"january\": 31, \"february\": 28, \"march\": 31, \"april\": 30, \"may\": 31,\\n\"june\": 30, \"july\": 31, \"august\": 31, \"september\": 30, \"october\": 31,\\n\"november\": 30, \"december\": 31}\\nHere is an example of a valid JSON object with one key \"a\" whose value is\\n{\"a\": 0}\\nHere is an example of a valid JSON object with one'}],\n",
       " 'mistral': [{'generated_text': 'Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\\nvalues are 0 and 1 respectively:\\n{\"a\": 0, \"b\": 1}\\nHere is an example of valid JSON object whose only keys are months in the\\nGregorian calendar and values are the number of days in those months.\\n{\"January\": 31, \"February\": 28, \"March\": 31, \"April\": 30, \"May\": 31,\\n\"June\": 30, \"July\": 31, \"August\": 31, \"September\": 30, \"October\": 31,\\n\"November\": 30, \"December\": 31}\\nHere is an example of a valid JSON object with a key \"a\" whose value is an\\narray of two strings \"b\" and'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = { name: pipeline(\"\"\"\n",
    "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
    "values are 0 and 1 respectively:\n",
    "{\"a\": 0, \"b\": 1}\n",
    "Here is an example of valid JSON object whose only keys are months in the\n",
    "Gregorian calendar and values are the number of days in those months.\n",
    "\"\"\"[1:-1]) for name, pipeline in pipelines.items()}\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a309db-ea16-468e-b0ba-eb13a364a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
      "values are 0 and 1 respectively:\n",
      "{\"a\": 0, \"b\": 1}\n",
      "Here is an example of valid JSON object whose only keys are months in the\n",
      "Gregorian calendar and values are the number of days in those months.\n",
      "{\"January\": 31, \"February\": 28, \"March\": 31, \"April\": 30, \"May\": 31,\n",
      "\"June\": 30, \"July\": 31, \"August\": 31, \"September\": 30, \"October\": 31,\n",
      "\"November\": 30, \"December\": 31}\n",
      "Here is an example of a valid JSON object with a key \"a\" whose value is an\n",
      "array of two strings \"b\" and\n",
      "\n",
      "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
      "values are 0 and 1 respectively:\n",
      "{\"a\": 0, \"b\": 1}\n",
      "Here is an example of valid JSON object whose only keys are months in the\n",
      "Gregorian calendar and values are the number of days in those months. The\n",
      "keys are not case sensitive:\n",
      "{\"january\": 31, \"february\": 28, \"march\": 31, \"april\": 30, \"may\": 31,\n",
      "\"june\": 30, \"july\": 31, \"august\": 31, \"september\": 30, \"october\": 31,\n",
      "\"november\": 30, \"december\": 31}\n",
      "Here is an example of a valid JSON object with one key \"a\" whose value is\n",
      "{\"a\": 0}\n",
      "Here is an example of a valid JSON object with one\n"
     ]
    }
   ],
   "source": [
    "print(out[\"mistral\"][0][\"generated_text\"] + \"\\n\")\n",
    "print(out[\"llama3\"][0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecaca12-39b9-4143-b228-6677fbd72401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
