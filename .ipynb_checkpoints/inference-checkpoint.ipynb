{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706f2c9f-8072-4f04-ac3e-dd1dcb2cf9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce58aed-7cb1-467c-a796-1cea508aa28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Loading checkpoint shards: 100%|██████| 2/2 [00:03<00:00,  1.54s/it]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "def get_pipeline(model_id, **kwargs):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        device_map=\"auto\",\n",
    "        max_new_tokens=128,\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "        },\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "pipelines = {\n",
    "    \"llama3\": get_pipeline(\"meta-llama/Llama-3.2-1B\"),\n",
    "    \"mistral\": get_pipeline(\"mistralai/Mistral-7B-v0.1\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12785086-8398-4cfb-9b2e-883c01c59490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████| 2/2 [00:54<00:00, 27.43s/it]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \" Prompt injection is a type of security vulnerability that can occur in web applications that use user input to generate dynamic content. It occurs when an attacker is able to inject malicious code into the user input, which is then executed by the web application.\\n\\nThe attacker can use various techniques to inject malicious code, such as SQL injection, cross-site scripting (XSS), and command injection. These techniques allow the attacker to manipulate the web application's behavior and potentially gain unauthorized access to sensitive data or perform malicious actions on behalf of the user.\\n\\nTo prevent prompt injection attacks,\"}\n"
     ]
    }
   ],
   "source": [
    "#this is used to chat with the Mistral-7B-Instruct-v0.1 mod\n",
    "pipe=pipeline(\"text-generation\", \"mistralai/Mistral-7B-Instruct-v0.1\") #the text generation pipeline has an automated pipeline for chat inputs\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"\"\"You are an assistant teaching at a university level,\n",
    "                Use technical and complex terms in your explanation \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": \"Please explain what a prompt injection is\"}\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])\n",
    "#this makes it so we won't have to use apply_chat_template, all we have to do is send messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbb0a14-0035-4bb0-bf14-beb2bbce33f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3880, in from_pretrained\n    raise EnvironmentError(\nOSError: meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llama_pipe\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m     {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease explain what a large language model is\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(llama_pipe(messages)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ac.cdavies/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3880, in from_pretrained\n    raise EnvironmentError(\nOSError: meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "llama_pipe=pipeline(\"text-generation\", \"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                {\"role\": \"user\", \"content\": \"Please explain what a large language model is\"}\n",
    "]\n",
    "print(llama_pipe(messages)[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28342549-401c-44fd-818e-ae5ba7e66190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama3': [{'generated_text': \"Hello World! I am a graduate student at the University of Maryland studying mathematics. I have a PhD in Mathematics and a BS in Mathematics from the University of Wisconsin-Madison. My research interests are in algebraic combinatorics, combinatorial number theory, and computational algebraic geometry. I am interested in learning about other people's research and trying to contribute to the community by helping to write papers or giving talks.\\nI am a graduate student at the University of Maryland studying mathematics. I have a PhD in Mathematics and a BS in Mathematics from the University of Wisconsin-Madison. My research interests are in algebraic combinatorics, combinator\"}],\n",
       " 'mistral': [{'generated_text': 'Hello World!\\n\\nI’m a 20-something year old girl who loves to travel, eat, and take pictures. I’m a full-time student at the University of Florida, and I’m currently studying abroad in London. I’m a huge fan of the outdoors, and I love to hike, bike, and run. I’m also a huge fan of the beach, and I love to swim and surf. I’m a huge fan of music, and I love to listen to all kinds of music. I’m a huge fan of food, and I love to eat all kinds of'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ name: pipeline(\"Hello World!\") for name, pipeline in pipelines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ef47a3-1b4e-4148-bf86-8f05b95d2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llama3': [{'generated_text': 'Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\\nvalues are 0 and 1 respectively:\\n{\"a\": 0, \"b\": 1}\\nHere is an example of valid JSON object whose only keys are months in the\\nGregorian calendar and values are the number of days in those months. The\\nkeys are not case sensitive:\\n{\"january\": 31, \"february\": 28, \"march\": 31, \"april\": 30, \"may\": 31,\\n\"june\": 30, \"july\": 31, \"august\": 31, \"september\": 30, \"october\": 31,\\n\"november\": 30, \"december\": 31}\\nHere is an example of a valid JSON object with one key \"a\" whose value is\\n{\"a\": 0}\\nHere is an example of a valid JSON object with one'}],\n",
       " 'mistral': [{'generated_text': 'Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\\nvalues are 0 and 1 respectively:\\n{\"a\": 0, \"b\": 1}\\nHere is an example of valid JSON object whose only keys are months in the\\nGregorian calendar and values are the number of days in those months.\\n{\"January\": 31, \"February\": 28, \"March\": 31, \"April\": 30, \"May\": 31,\\n\"June\": 30, \"July\": 31, \"August\": 31, \"September\": 30, \"October\": 31,\\n\"November\": 30, \"December\": 31}\\nHere is an example of a valid JSON object with a key \"a\" whose value is an\\narray of two strings \"b\" and'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = { name: pipeline(\"\"\"\n",
    "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
    "values are 0 and 1 respectively:\n",
    "{\"a\": 0, \"b\": 1}\n",
    "Here is an example of valid JSON object whose only keys are months in the\n",
    "Gregorian calendar and values are the number of days in those months.\n",
    "\"\"\"[1:-1]) for name, pipeline in pipelines.items()}\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a309db-ea16-468e-b0ba-eb13a364a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
      "values are 0 and 1 respectively:\n",
      "{\"a\": 0, \"b\": 1}\n",
      "Here is an example of valid JSON object whose only keys are months in the\n",
      "Gregorian calendar and values are the number of days in those months.\n",
      "{\"January\": 31, \"February\": 28, \"March\": 31, \"April\": 30, \"May\": 31,\n",
      "\"June\": 30, \"July\": 31, \"August\": 31, \"September\": 30, \"October\": 31,\n",
      "\"November\": 30, \"December\": 31}\n",
      "Here is an example of a valid JSON object with a key \"a\" whose value is an\n",
      "array of two strings \"b\" and\n",
      "\n",
      "Here is an example of a valid JSON object with two keys \"a\" and \"b\" whose\n",
      "values are 0 and 1 respectively:\n",
      "{\"a\": 0, \"b\": 1}\n",
      "Here is an example of valid JSON object whose only keys are months in the\n",
      "Gregorian calendar and values are the number of days in those months. The\n",
      "keys are not case sensitive:\n",
      "{\"january\": 31, \"february\": 28, \"march\": 31, \"april\": 30, \"may\": 31,\n",
      "\"june\": 30, \"july\": 31, \"august\": 31, \"september\": 30, \"october\": 31,\n",
      "\"november\": 30, \"december\": 31}\n",
      "Here is an example of a valid JSON object with one key \"a\" whose value is\n",
      "{\"a\": 0}\n",
      "Here is an example of a valid JSON object with one\n"
     ]
    }
   ],
   "source": [
    "print(out[\"mistral\"][0][\"generated_text\"] + \"\\n\")\n",
    "print(out[\"llama3\"][0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c003f1c-6a13-4ef1-801a-0b520927ecc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
